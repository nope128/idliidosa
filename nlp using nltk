import nltk
nltk.download('punkt')       # you’ve already done this
nltk.download('punkt_tab')   # the one that’s still missing

from nltk.tokenize import sent_tokenize

text = "The quick brown fox jumps over the lazy dog. Dogs are not always lazy!"
sent_tokens = sent_tokenize(text)
print(sent_tokens)

import nltk
import string
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords, wordnet
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk import pos_tag

# Download required NLTK data files
nltk.download('punkt')                     # tokenizers
nltk.download('stopwords')                 # list of stopwords
nltk.download('wordnet')                   # WordNet lexical database
nltk.download('omw-1.4')                   # WordNet multilingual data (needed by lemmatizer)
nltk.download('averaged_perceptron_tagger')# POS tagger data

# Helper to map NLTK POS tags to WordNet POS tags
def get_wordnet_pos(treebank_tag):
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN  # default to noun

# Sample text
text = "The quick brown fox jumps over the lazy dog. Dogs are not always lazy!"

# 1. Sentence Tokenization
sent_tokens = sent_tokenize(text)
print("Sentence Tokens:\n", sent_tokens)

# 2. Word Tokenization
word_tokens = word_tokenize(text)
print("\nWord Tokens:\n", word_tokens)

# 3. Remove punctuation
words_no_punct = [w for w in word_tokens if w not in string.punctuation]
print("\nWithout Punctuation:\n", words_no_punct)

# 4. Remove stopwords (lowercased for matching)
stop_words = set(stopwords.words('english'))
filtered_words = [w for w in words_no_punct if w.lower() not in stop_words]
print("\nWithout Stopwords:\n", filtered_words)

# 5. Stemming
stemmer = PorterStemmer()
stemmed = [stemmer.stem(w) for w in filtered_words]
print("\nStemmed Words:\n", stemmed)
nltk.download('averaged_perceptron_tagger_eng')
# 6. POS Tagging (on filtered words)
pos_tags = pos_tag(filtered_words)
print("\nPOS Tags:\n", pos_tags)

# 7. Lemmatization (using POS)
lemmatizer = WordNetLemmatizer()
lemmatized = [
    lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag))
    for word, tag in pos_tags
]
print("\nLemmatized Words:\n", lemmatized)
